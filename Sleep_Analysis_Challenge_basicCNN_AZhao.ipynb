{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sleep_Analysis_Challenge_basicCNN_AZhao.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azhao20/Week2_Public/blob/master/Sleep_Analysis_Challenge_basicCNN_AZhao.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB6YunD0Gdgc",
        "colab_type": "text"
      },
      "source": [
        "# \"You Snooze, You Win\" Challenge\n",
        "\n",
        "Every year, the [PhysioNet/CinC (Computing in Cardiology) Challenge](https://www.physionet.org/challenge/) invites \"participants to tackle clinically interesting problems that are either unsolved or not well-solved.\" For this year's week 2 machine learning challenge, BWSI has revived a past PhysioNet challenge based on sleep classification.\n",
        "\n",
        "This year's challenge focuses on the classification of nonarousal and arousal timeframes. If you would like to understand the biological implications of the challenge, we recommend reading PhysioNet's [introduction](https://physionet.org/challenge/2018/) of the challenge.\n",
        "\n",
        "For this challenge, you will classify samples into 5 classes (Arousal, NREM1, NREM2, NREM3, REM). Each sample consists of seven physiological signals (O2-M1, E1-M2, Chin1-Chin2, ABD, CHEST, AIRFLOW, ECG) measured at 200 Hz over a 60 second period (12000 timepoints). In this notebook, we provide code to import the data, visualize sample signals, implement an example classifier, and 'score' your model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ctSzKHoGr1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Import libraries ###\n",
        "\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "\n",
        "#set default plotting fonts\n",
        "font = {'family' : 'sans-serif',\n",
        "        'weight' : 'normal',\n",
        "        'size'   : 20}\n",
        "\n",
        "matplotlib.rc('font', **font)\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "\n",
        "#data preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#garbage collection (for saving RAM during training)\n",
        "import gc\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XxIE6ZwGvjh",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Dataset\n",
        "\n",
        "This dataset is a modified version of the PhysioNet/CinC Challenge data, which were contributed by the Massachusetts General Hospital’s Computational Clinical Neurophysiology Laboratory, and the Clinical Data Animation Laboratory.\n",
        "***\n",
        "**Class labels:**\n",
        "- 0 = Arousal\n",
        "- 1 = NREM1\n",
        "- 2 = NREM2\n",
        "- 3 = NREM3\n",
        "- 4 = REM\n",
        "***\n",
        "**Class descriptions:**\n",
        "\n",
        "<img src=\"https://github.com/BeaverWorksMedlytics2020/Data_Public/blob/master/Images/Week2/sleepStagesTable.svg?raw=true\">\n",
        "\n",
        "***\n",
        "**Physiological signal description:**\n",
        "\n",
        "O2-M1 - posterior brain activity (electroencephalography)\n",
        "\n",
        "E1-M2 - left eye activity (electrooculography)\n",
        "\n",
        "Chin1-Chin2 - chin movement (electromyography)\n",
        "\n",
        "ABD - abdominal movement (electromyography)\n",
        "\n",
        "CHEST - chest movement (electromyography)\n",
        "\n",
        "AIRFLOW - respiratory airflow\n",
        "\n",
        "ECG - cardiac activity (electrocardiography)\n",
        "***\n",
        "Run both cell blocks to get the challenge data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfgbZPNYziXt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "538f630f-fb8c-4448-9714-7bf91b39009f"
      },
      "source": [
        "# Clone repo and move into data directory (only run this once)\n",
        "!git clone https://github.com/BeaverWorksMedlytics2020/Data_Public\n",
        "os.chdir('./Data_Public/ChallengeProjects/Week2/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Data_Public'...\n",
            "remote: Enumerating objects: 144, done.\u001b[K\n",
            "remote: Counting objects: 100% (144/144), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 5269 (delta 54), reused 60 (delta 23), pack-reused 5125\u001b[K\n",
            "Receiving objects: 100% (5269/5269), 1.11 GiB | 15.46 MiB/s, done.\n",
            "Resolving deltas: 100% (68/68), done.\n",
            "Checking out files: 100% (5130/5130), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qANF2BvQG2m3",
        "colab_type": "text"
      },
      "source": [
        "## Loading Data in Memory\n",
        "Run the cell below to extract the raw training and test data. It may take a minute or two to run through. Here are the variables containing the data you will get:\n",
        "\n",
        "* **data_train**: np array shape (4000, 12000, 7). Contains 4000 samples (60s each) of 12000 data points (200Hz x 60s), for 7 different signals. \n",
        "* **labels_train**: np array shape (4000,). Contains ground truth labels for data_train. The order of the labels corresponds to the order of the training data.\n",
        "* **ID_train**: list of 4000 unique IDs. The order of the IDs corresponds to the order of the training data. \n",
        "* **data_test**: np array shape (1000, 12000, 7). Contains 1000 samples (60s each) of 12000 data points (200Hz x 60s), for 7 different signals.\n",
        "* **ID_test**: list of 1000 unique IDs. The order of the IDs corresponds to the order of the training data.\n",
        "\n",
        "We encourage you to print each of these variables to see what they look like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Rw8inOvG5QP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a0e21b9e-8c6f-402c-a0d5-8856b3091aab"
      },
      "source": [
        "### Run once to import data ###\n",
        "\n",
        "def get_file_locs():\n",
        "    '''\n",
        "    find all files in 'training' and 'test' directories and put their names \n",
        "    under 'training' and 'test' keys in the file_dict dictionary\n",
        "    '''\n",
        "\n",
        "    file_dict = {'training':[], 'test':[]}\n",
        "    for data_type in file_dict:\n",
        "        for file in os.listdir('./' + data_type):\n",
        "            file_dict[data_type].append(data_type + '/' + file)\n",
        "    \n",
        "    return file_dict\n",
        "\n",
        "def get_sample_data(data_type, id_number):\n",
        "    '''\n",
        "    get signal data, label, and filename associated with given data type and index num\n",
        "\n",
        "    parameters:\n",
        "\n",
        "     data_type -- Dictates whether sample comes from training set or test set.\n",
        "                 This input must be either 'training' or 'test' (defaults to 'training')\n",
        "\n",
        "     id_number -- Which sample ID should be returned? Must be 0-3999 if data_type is 'training'\n",
        "                 or 0-999 if data_type is 'test' (defaults to random integer from 0-999)\n",
        "  \n",
        "    returns:\n",
        "\n",
        "     sample_data -- dataframe with 1 row and 2 columns-- column \"Signal\" contains a series object \n",
        "                    and column \"Label\" contains numeric label for that sample\n",
        "    '''\n",
        "    file = './' + data_type + '/' + str(id_number) + '.xz'\n",
        "\n",
        "    #sample_data is a dataframe with 1 row and 2 columns--\n",
        "    #\"Signal\" (contains a series object) and \"Label\" (contains numeric label)\n",
        "    sample_data = pd.read_pickle('./' + file)\n",
        "\n",
        "    return sample_data, file.split('/')[2]\n",
        "\n",
        "file_dict = get_file_locs()\n",
        "print(f\"{len(file_dict['training'])} training samples found, {len(file_dict['test'])} test samples found\")\n",
        "\n",
        "data_train = np.zeros((4000, 12000, 7))\n",
        "labels_train = np.zeros(4000)\n",
        "ID_train = []\n",
        "for i in range(4000):\n",
        "  sample_data, ID = get_sample_data('training', i)\n",
        "  data_train[i] = np.array(list(sample_data['Signal']), dtype=np.float).reshape(12000, 7)\n",
        "  labels_train[i] = np.array(list(sample_data['Label']), dtype=np.float)\n",
        "  ID_train.append(ID)\n",
        "  if(i%500==0):\n",
        "    print('Loading training sample ' + str(i))\n",
        "  \n",
        "data_test = np.zeros((1000, 12000, 7))\n",
        "ID_test = []\n",
        "for i in range(1000):\n",
        "  sample_data, ID = get_sample_data('test', i)\n",
        "  data_test[i] = np.array(list(sample_data['Signal']), dtype=np.float).reshape(12000, 7)\n",
        "  ID_test.append(ID)\n",
        "  if(i%500==0):\n",
        "    print('Loading test sample ' + str(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4000 training samples found, 1000 test samples found\n",
            "Loading training sample 0\n",
            "Loading training sample 500\n",
            "Loading training sample 1000\n",
            "Loading training sample 1500\n",
            "Loading training sample 2000\n",
            "Loading training sample 2500\n",
            "Loading training sample 3000\n",
            "Loading training sample 3500\n",
            "Loading test sample 0\n",
            "Loading test sample 500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVNgpgciw6p4",
        "colab_type": "text"
      },
      "source": [
        "## Create One-Hot Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XN0BBD3fHKRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Create label array for all training samples using categorical datatype ###\n",
        "\n",
        "train_labels = np.ndarray(shape = (1, 4000))\n",
        "\n",
        "#set labels to integers first\n",
        "for i in range(4000):\n",
        "    train_labels[0][i] = i//800 # This is a way to label each entry (since classes are in order)\n",
        "\n",
        "#convert labels to onehot, ensure type is float32\n",
        "train_labels = tf.keras.utils.to_categorical(train_labels[0], 5).astype(np.float32)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIzqcXmJBHdX",
        "colab_type": "text"
      },
      "source": [
        "## Shuffle and Partition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "buuR3HH9yTOw",
        "colab": {}
      },
      "source": [
        "### Shuffle and partition all train data\n",
        "\n",
        "#(Training data is ordered by default so shuffling before partitioning is important)\n",
        "\n",
        "#--Shuffle data_train--\n",
        "#(Note that data is only shuffled in first dimension, which is what we want)\n",
        "data_train, train_labels = shuffle(data_train, train_labels, random_state = 25, stratify = train_labels)\n",
        "\n",
        "#--Scale all labeled data in data_train--\n",
        "\n",
        "#initialize standard scaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "#Standard scaler is meant for 2D arrays, so we reshape, scale, and then reshape again\n",
        "reshaped_X_train = data_train.reshape((data_train.shape[0]*data_train.shape[1], data_train.shape[2])).copy()\n",
        "reshaped_X_train = scaler.fit_transform(reshaped_X_train)\n",
        "data_train = reshaped_X_train.reshape((data_train.shape[0], data_train.shape[1], data_train.shape[2]))\n",
        "\n",
        "del reshaped_X_train #get rid of large temporary variable\n",
        "\n",
        "#--Scale unlabeled test data--\n",
        "#Because we scaled labeled data before training, we need to also scale test data --\n",
        "\n",
        "#Standard scaler is meant for 2D arrays, so we reshape, apply scaling, and then \n",
        "#reshape again to get back to original\n",
        "reshaped_X_test = data_test.reshape((data_test.shape[0]*data_test.shape[1], data_test.shape[2])).copy()\n",
        "reshaped_X_test = scaler.transform(reshaped_X_test)\n",
        "data_test = reshaped_X_test.reshape((data_test.shape[0], data_test.shape[1], data_test.shape[2]))\n",
        "\n",
        "del reshaped_X_test #get rid of large temporary variable\n",
        "\n",
        "#--create 3 partitions of provided training data--\n",
        "# Note we are breaking up provided labeled data into training, validation, and \"mock test\" sets\n",
        "\n",
        "val_size = 1000\n",
        "mocktest_size = 500\n",
        "\n",
        "mocktest_data = data_train[0:mocktest_size, :, :]\n",
        "mocktest_labels = train_labels[0:mocktest_size, :]\n",
        "\n",
        "val_data = data_train[mocktest_size:mocktest_size+val_size, :, :]\n",
        "val_labels = train_labels[mocktest_size:mocktest_size+val_size, :]\n",
        "\n",
        "partial_train_data = data_train[mocktest_size+val_size:,:,:]\n",
        "tr_labels = train_labels[mocktest_size+val_size:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOcj_uuUHP3Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "320f0215-888c-48bb-9a86-5b16b127bf1d"
      },
      "source": [
        "### Run every time you change set of parameters ###\n",
        "\n",
        "class garbage_collect_callback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    gc.collect()\n",
        "\n",
        "#this will reset your model if you want to make changes, and just see the changes - JQ\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "\"\"\" Modify to your heart's and algorithm's content ^_^ \"\"\"\n",
        "\n",
        "model.add(tf.keras.layers.Conv1D(filters = 14, kernel_size = 3, padding = 'valid',\n",
        "                                activation=tf.nn.relu, \n",
        "                                input_shape=(partial_train_data.shape[1],partial_train_data.shape[2])))\n",
        "model.add(tf.keras.layers.Conv1D(filters = 16, kernel_size = 3, padding = 'valid',\n",
        "                                activation=tf.nn.relu, \n",
        "                                input_shape=(partial_train_data.shape[1],partial_train_data.shape[2])))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.1))\n",
        "model.add(tf.keras.layers.Conv1D(filters = 16, kernel_size = 3, padding = 'valid',\n",
        "                                activation=tf.nn.relu, \n",
        "                                input_shape=(partial_train_data.shape[1],partial_train_data.shape[2])))\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 5, padding = 'valid',\n",
        "                                activation=tf.nn.relu, \n",
        "                                input_shape=(partial_train_data.shape[1],partial_train_data.shape[2])))\n",
        "model.add(tf.keras.layers.Dropout(rate=0.1))\n",
        "model.add(tf.keras.layers.Conv1D(filters = 32, kernel_size = 5, padding = 'valid', #I don't know about having 4—the function was comparable with like 1. I'd recommend trying 2-3?\n",
        "                                activation=tf.nn.relu, \n",
        "                                input_shape=(partial_train_data.shape[1],partial_train_data.shape[2])))\n",
        "\n",
        "#take maximum activation value from each convolution result and pass to next layer\n",
        "model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
        "\n",
        "#for val in range(7):\n",
        "#insert a dense layer with 64 units\n",
        "  #model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
        "model.add(tf.keras.layers.Dense(256, activation=tf.nn.relu))\n",
        "\n",
        "# we should end with a softmax to ensure outputs behave like probabilities\n",
        "#(i.e. sum to 1)\n",
        "model.add(tf.keras.layers.Dense(5, activation=tf.nn.softmax)) \n",
        "\n",
        "#opt = tf.keras.optimizers.RMSprop(learning_rate=0.001) #Definitely change this\n",
        "#Another potential optimizer\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate=0.001)\n",
        "\n",
        "#opt = tf.keras.optimizers.\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history = model.fit(partial_train_data, # Train examples\n",
        "        tr_labels, # Train labels\n",
        "        epochs=100, # number of epochs (passes through data during training)\n",
        "        batch_size=200, # number of points to consider in each optimizer iteration\n",
        "        callbacks = [garbage_collect_callback()],\n",
        "        validation_data=(val_data, val_labels), #data to use for validation\n",
        "        verbose=1) #will print information about optimization process"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 11998, 14)         308       \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 11996, 16)         688       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 11996, 16)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 11994, 16)         784       \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 11990, 32)         2592      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 11990, 32)         0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 11986, 32)         5152      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 5)                 325       \n",
            "=================================================================\n",
            "Total params: 24,441\n",
            "Trainable params: 24,441\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "13/13 [==============================] - 6s 486ms/step - loss: 1.6007 - accuracy: 0.2112 - val_loss: 1.6029 - val_accuracy: 0.2000\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 6s 464ms/step - loss: 1.5926 - accuracy: 0.2400 - val_loss: 1.5981 - val_accuracy: 0.2220\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 6s 466ms/step - loss: 1.5879 - accuracy: 0.2500 - val_loss: 1.5965 - val_accuracy: 0.2230\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 6s 470ms/step - loss: 1.5823 - accuracy: 0.2512 - val_loss: 1.5931 - val_accuracy: 0.2170\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 6s 465ms/step - loss: 1.5789 - accuracy: 0.2548 - val_loss: 1.5925 - val_accuracy: 0.2270\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 6s 466ms/step - loss: 1.5714 - accuracy: 0.2944 - val_loss: 1.5844 - val_accuracy: 0.2500\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 6s 465ms/step - loss: 1.5624 - accuracy: 0.2872 - val_loss: 1.5812 - val_accuracy: 0.2700\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 6s 461ms/step - loss: 1.5522 - accuracy: 0.3116 - val_loss: 1.5781 - val_accuracy: 0.3070\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 6s 464ms/step - loss: 1.5452 - accuracy: 0.3244 - val_loss: 1.5798 - val_accuracy: 0.2960\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 6s 465ms/step - loss: 1.5384 - accuracy: 0.3336 - val_loss: 1.5690 - val_accuracy: 0.3270\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 6s 465ms/step - loss: 1.5263 - accuracy: 0.3668 - val_loss: 1.5691 - val_accuracy: 0.3280\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 6s 460ms/step - loss: 1.5120 - accuracy: 0.3876 - val_loss: 1.5488 - val_accuracy: 0.3870\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 6s 461ms/step - loss: 1.4894 - accuracy: 0.4008 - val_loss: 1.5105 - val_accuracy: 0.4120\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 6s 463ms/step - loss: 1.4441 - accuracy: 0.4304 - val_loss: 1.4869 - val_accuracy: 0.4200\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 6s 459ms/step - loss: 1.4049 - accuracy: 0.4360 - val_loss: 1.4445 - val_accuracy: 0.4190\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 6s 460ms/step - loss: 1.3569 - accuracy: 0.4400 - val_loss: 1.3742 - val_accuracy: 0.4540\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 6s 465ms/step - loss: 1.3203 - accuracy: 0.4544 - val_loss: 1.3998 - val_accuracy: 0.4370\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 6s 463ms/step - loss: 1.3045 - accuracy: 0.4612 - val_loss: 1.3671 - val_accuracy: 0.4670\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 6s 461ms/step - loss: 1.2662 - accuracy: 0.4792 - val_loss: 1.3031 - val_accuracy: 0.4810\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 6s 461ms/step - loss: 1.2482 - accuracy: 0.4840 - val_loss: 1.3113 - val_accuracy: 0.4930\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 6s 460ms/step - loss: 1.2326 - accuracy: 0.4904 - val_loss: 1.3273 - val_accuracy: 0.4770\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 6s 461ms/step - loss: 1.2320 - accuracy: 0.4952 - val_loss: 1.2993 - val_accuracy: 0.4950\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 6s 462ms/step - loss: 1.2110 - accuracy: 0.4964 - val_loss: 1.2880 - val_accuracy: 0.4860\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 6s 461ms/step - loss: 1.2007 - accuracy: 0.4932 - val_loss: 1.2760 - val_accuracy: 0.4990\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 6s 465ms/step - loss: 1.1993 - accuracy: 0.5072 - val_loss: 1.2792 - val_accuracy: 0.4910\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 6s 460ms/step - loss: 1.1882 - accuracy: 0.5008 - val_loss: 1.2898 - val_accuracy: 0.5030\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 6s 466ms/step - loss: 1.1890 - accuracy: 0.5136 - val_loss: 1.2488 - val_accuracy: 0.4970\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 6s 467ms/step - loss: 1.1791 - accuracy: 0.5056 - val_loss: 1.2757 - val_accuracy: 0.4920\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 6s 463ms/step - loss: 1.1748 - accuracy: 0.5208 - val_loss: 1.2502 - val_accuracy: 0.5000\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 6s 462ms/step - loss: 1.1721 - accuracy: 0.5156 - val_loss: 1.2704 - val_accuracy: 0.4930\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 6s 465ms/step - loss: 1.1601 - accuracy: 0.5188 - val_loss: 1.2836 - val_accuracy: 0.4740\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 6s 467ms/step - loss: 1.1578 - accuracy: 0.5156 - val_loss: 1.2589 - val_accuracy: 0.5040\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 6s 462ms/step - loss: 1.1501 - accuracy: 0.5200 - val_loss: 1.2669 - val_accuracy: 0.4790\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 6s 464ms/step - loss: 1.1544 - accuracy: 0.5224 - val_loss: 1.2351 - val_accuracy: 0.5180\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 6s 465ms/step - loss: 1.1509 - accuracy: 0.5180 - val_loss: 1.2846 - val_accuracy: 0.4660\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 6s 464ms/step - loss: 1.1417 - accuracy: 0.5196 - val_loss: 1.3250 - val_accuracy: 0.4360\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 6s 463ms/step - loss: 1.1397 - accuracy: 0.5288 - val_loss: 1.2710 - val_accuracy: 0.4910\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 6s 463ms/step - loss: 1.1252 - accuracy: 0.5360 - val_loss: 1.3059 - val_accuracy: 0.4580\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 6s 463ms/step - loss: 1.1275 - accuracy: 0.5396 - val_loss: 1.2202 - val_accuracy: 0.5150\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 6s 463ms/step - loss: 1.1345 - accuracy: 0.5268 - val_loss: 1.3076 - val_accuracy: 0.4430\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 6s 457ms/step - loss: 1.1218 - accuracy: 0.5364 - val_loss: 1.3044 - val_accuracy: 0.4450\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 6s 457ms/step - loss: 1.1084 - accuracy: 0.5416 - val_loss: 1.2592 - val_accuracy: 0.4810\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 6s 460ms/step - loss: 1.0977 - accuracy: 0.5552 - val_loss: 1.2556 - val_accuracy: 0.4760\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 6s 460ms/step - loss: 1.1026 - accuracy: 0.5424 - val_loss: 1.3205 - val_accuracy: 0.4360\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 6s 465ms/step - loss: 1.1025 - accuracy: 0.5316 - val_loss: 1.3008 - val_accuracy: 0.4560\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 6s 465ms/step - loss: 1.0965 - accuracy: 0.5484 - val_loss: 1.3621 - val_accuracy: 0.4310\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 6s 465ms/step - loss: 1.0971 - accuracy: 0.5468 - val_loss: 1.2659 - val_accuracy: 0.4700\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 6s 461ms/step - loss: 1.0943 - accuracy: 0.5448 - val_loss: 1.3358 - val_accuracy: 0.4380\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 6s 461ms/step - loss: 1.0797 - accuracy: 0.5572 - val_loss: 1.2717 - val_accuracy: 0.4670\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 6s 467ms/step - loss: 1.0818 - accuracy: 0.5584 - val_loss: 1.3634 - val_accuracy: 0.4330\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 6s 458ms/step - loss: 1.0753 - accuracy: 0.5648 - val_loss: 1.2977 - val_accuracy: 0.4610\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 6s 464ms/step - loss: 1.0718 - accuracy: 0.5696 - val_loss: 1.3480 - val_accuracy: 0.4460\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 6s 465ms/step - loss: 1.0849 - accuracy: 0.5544 - val_loss: 1.3608 - val_accuracy: 0.4340\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 6s 461ms/step - loss: 1.0701 - accuracy: 0.5588 - val_loss: 1.2802 - val_accuracy: 0.4720\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 6s 464ms/step - loss: 1.0616 - accuracy: 0.5708 - val_loss: 1.4574 - val_accuracy: 0.4170\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 6s 462ms/step - loss: 1.0626 - accuracy: 0.5696 - val_loss: 1.2285 - val_accuracy: 0.4850\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 6s 465ms/step - loss: 1.0592 - accuracy: 0.5724 - val_loss: 1.4193 - val_accuracy: 0.4210\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 6s 462ms/step - loss: 1.0550 - accuracy: 0.5616 - val_loss: 1.3754 - val_accuracy: 0.4380\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 6s 464ms/step - loss: 1.0409 - accuracy: 0.5836 - val_loss: 1.3763 - val_accuracy: 0.4440\n",
            "Epoch 60/100\n",
            " 9/13 [===================>..........] - ETA: 1s - loss: 1.0410 - accuracy: 0.5778"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-333b0219366d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgarbage_collect_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#data to use for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         verbose=1) #will print information about optimization process\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhhsAXXJHRvJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Run whenever you want to train and validate your model ###\n",
        "\n",
        "\"\"\"\n",
        "EPOCHS (int) the number of times the optimization algorithm passes\n",
        "through the full dataset (calculating errors and derivatives) to update weights\n",
        "(One pass through the data is called an \"epoch\")\n",
        "\"\"\"\n",
        "\n",
        "#This function is called after each epoch\n",
        "#(It will ensure that your training process does not consume all available RAM)\n",
        "class garbage_collect_callback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    gc.collect()\n",
        "\n",
        "history = model.fit(partial_train_data, # Train examples\n",
        "          tr_labels, # Train labels\n",
        "          epochs=20, # number of epochs (passes through data during training)\n",
        "          batch_size=600, # number of points to consider in each optimizer iteration\n",
        "          callbacks = [garbage_collect_callback()],\n",
        "          validation_data=(val_data, val_labels), #data to use for validation\n",
        "          verbose=1) #will print information about optimization process\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRk6Xh_LHTXB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Run once after you have finished training your model ###\n",
        "#Reminder: Please ensure order of test data points has not been changed.\n",
        "\n",
        "#-- Evaluate model for test data --\n",
        "test_pred = model.predict(data_test)\n",
        "test_output = np.ndarray(shape = (1000, 6))\n",
        "\n",
        "#-- Write test output to dataframe and save to pickle file --\n",
        "test_dataframe = pd.DataFrame(test_output)\n",
        "\n",
        "file = 'unladen_swallow.xz'\n",
        "test_dataframe.to_pickle(file)\n",
        "test_dataframe = pd.DataFrame(test_output)\n",
        "os.listdir('.')\n",
        "files.download(file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bduvOuTtHVfK",
        "colab_type": "text"
      },
      "source": [
        "## Submitting Your Model\n",
        "\n",
        "After training your classifier, run it on the test data to generate your predictions. Each class for a test sample should have an associated probability (between 0 and 1). Below are the parameters for the prediction format and export:\n",
        "\n",
        "- Your predictions should be in a pandas dataframe with 5 columns (classes) and 1000 rows (samples). Note that your predictions must follow the original test sample order (0.xz, 1.xz, 2.xz, ...). You only need to worry about this if you shuffled the test samples or stored the samples in an unordered data structure (dictionaries and sets). If this is the case, you should 1) add a separate column in your pandas dataframe with the file number for each sample; 2) sort the dataframe using this column; and 3) drop the column. These steps have been noted in the code below.\n",
        "- The predictions dataframe should be exported as an .xz file using dataframe.to_pickle() followed by files.download().\n",
        "\n",
        "Example code of the prediction format and export is presented in the cell block below. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TiEUxHcZz_D",
        "colab_type": "text"
      },
      "source": [
        "Your model will be evaluated on Area Under the ROC Curve (ROCAUC), Matthews Correlation Coefficient (MCC) and creativity. There will be a \"winning\" group for each of these categories.\n",
        "\n",
        "If you are finished early, consider trying other ML algorithms and/or implementing multiple feature extraction methods. You can also help other groups if you finish early."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDQWbhUK6cMt",
        "colab_type": "text"
      },
      "source": [
        "## How Your Model Will Be Evaluated\n",
        "\n",
        "- **Area Under the ROC Curve (AUCROC)**: The receiver operating characteristic (ROC) curve plots the true positive rate (sensitivity/recall) against the false positive rate (fall-out) at many decision threshold settings. The area under the curve (AUC) measures discrimination, the classifier's ability to correctly identify samples from the \"positive\" and \"negative\" cases. Intuitively, AUC is the probability that a randomly chosen \"positive\" sample will be labeled as \"more positive\" than a randomly chosen \"negative\" sample. In the case of a multi-class ROC curve, each class is considered separately before taking the weighted average of all the class results. Simply put, the class under consideration is labeled as \"positive\" while all other classes are labeled as \"negative.\" Below is the multi-class ROC curve for the example classifier. The AUCROC score should be between 0 and 1, in which 0.5 is random classification and 1 is perfect classification.\n",
        "\n",
        "<img src=\"https://github.com/BeaverWorksMedlytics2020/Data_Public/blob/master/Images/Week2/MultiClassRocCurve_exampleClassifier.png?raw=true\" width=\"600\" height=\"500\">\n",
        "\n",
        "- **Matthews Correlation Coefficient (MCC)**: The MCC measures the quality of binary classifications, irrespective of the class sizes. Importantly, it is typically regarded as a balanced measure since it considers all values in the 2x2 contingency table (TP, FP, TN, FN). For this challenge, the binary classes will be \"Arousal\" (Arousal) and \"Nonarousal\" (NREM1, NREM2, NREM3, REM). The MCC score should be between -1 and 1, in which 0 is random classification and 1 is perfect classification.\n",
        "\n",
        " ![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/5caa90fc15105b74b59a30bbc9cc2e5bd43a13b7)\n",
        "\n",
        "Using these metrics, the example classifier has the following scores on test data:\n",
        "- AUCROC: 0.727\n",
        "- MCC: 0.163\n",
        "- Creativity: ( ͡° ͜ʖ ͡°)\n",
        "\n",
        "Below is the code used to calculate the AUCROC and MCC metrics when evaluating your classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOpioHig8688",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_pred = pd.DataFrame(model.predict(mocktest_data))\n",
        "test_predict = test_pred.idxmax(axis=1)\n",
        "test_labels = [ np.where(label==1)[0][0] for label in mocktest_labels]\n",
        "test_labels_one_hot = pd.DataFrame(mocktest_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIlfwfksLp-j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fpr = {}\n",
        "tpr = {}\n",
        "roc_auc = {}\n",
        "\n",
        "\"\"\" Initalize key reference dictionaries \"\"\"\n",
        "sig_dict = {0:'O2-M1', 1:'E1-M2', 2:'Chin1-Chin2', 3:'ABD', 4:'CHEST', 5:'AIRFLOW', 6:'ECG'}\n",
        "sig_type_dict = {0:'Time (s)', 1:'Frequency (Hz)'}\n",
        "stage_dict = {0:'Arousal', 1:'NREM1', 2:'NREM2', 3:'NREM3', 4:'REM'}\n",
        "\n",
        "plt.figure(figsize=(14,10))\n",
        "for i in range(5):\n",
        "    fpr[i], tpr[i], _ = metrics.roc_curve(test_labels_one_hot.iloc[:, i], test_pred.iloc[:, i])\n",
        "    roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
        "    plt.plot(fpr[i], tpr[i], label = stage_dict[i] + ', ' + str(i))\n",
        "\n",
        "plt.plot([0, 1], [0, 1])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Multi-Class ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = metrics.roc_curve(test_labels_one_hot.values.ravel(), test_pred.values.ravel())\n",
        "roc_auc_agg = metrics.auc(fpr[\"micro\"], tpr[\"micro\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO1I5lme8oya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "for i in range(test_pred.shape[0]):\n",
        "    if test_predict.iloc[i]==0: y_pred.append(1)\n",
        "    else: y_pred.append(-1)\n",
        "    if test_labels[i]==0: y_true.append(1)\n",
        "    else: y_true.append(-1)\n",
        "mcc = metrics.matthews_corrcoef(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo7edcTTC6yD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(roc_auc_agg, mcc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6j_GmhqpqLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}